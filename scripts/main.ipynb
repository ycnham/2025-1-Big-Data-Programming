{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main_title",
   "metadata": {},
   "source": [
    "# ì „ê¸°ì°¨ ì¶©ì „ì†Œ ìµœì  ì…ì§€ ë¶„ì„ - í†µí•© ë²„ì „\n",
    "## ycnham + mg ë¸Œëœì¹˜ í†µí•© ì‹¤í–‰\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‘ ë¸Œëœì¹˜ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©í•˜ì—¬ ì™„ì „í•œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” í†µí•© ê¸°ëŠ¥:\n",
    "- **ImprovedDemandScoreCalculator**: ì—…ì¢…ë³„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê³ ê¸‰ ìˆ˜ìš” ì ìˆ˜ ê³„ì‚°\n",
    "- **ì´ì¤‘ ë°±ì—… ì‹œìŠ¤í…œ**: ê³ ê¸‰ ë°©ì‹ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì „í™˜\n",
    "- **ì™„ì „í•œ ë¶„ì„ íŒŒì´í”„ë¼ì¸**: ì „ì²˜ë¦¬ â†’ ëª¨ë¸ë§ â†’ ìµœì í™” â†’ í‰ê°€\n",
    "- **ì•ˆì •ì„± ë³´ì¥**: mg ë¸Œëœì¹˜ì˜ ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ycnham ë¸Œëœì¹˜ì˜ ê³ ê¸‰ ê¸°ëŠ¥ ê²°í•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba51443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹œì‘...\n",
      "âœ… xgboost ì´ë¯¸ ì„¤ì¹˜ë¨\n",
      "ğŸ“¦ scikit-learn ì„¤ì¹˜ ì¤‘...\n",
      "âœ… scikit-learn ì„¤ì¹˜ ì™„ë£Œ\n",
      "âœ… pulp ì´ë¯¸ ì„¤ì¹˜ë¨\n",
      "âœ… folium ì´ë¯¸ ì„¤ì¹˜ë¨\n",
      "âœ… tqdm ì´ë¯¸ ì„¤ì¹˜ë¨\n",
      "ğŸ“¦ geopy ì„¤ì¹˜ ì¤‘...\n",
      "âœ… geopy ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ‰ ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ì…€ 1: í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ëª©ë¡\n",
    "required_packages = [\n",
    "    'xgboost',\n",
    "    'scikit-learn', \n",
    "    'pulp',\n",
    "    'folium',\n",
    "    'tqdm',\n",
    "    'geopy'\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹œì‘...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"âœ… {package} ì´ë¯¸ ì„¤ì¹˜ë¨\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...\")\n",
    "        install_package(package)\n",
    "        print(f\"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "\n",
    "print(\"ğŸ‰ ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8b97a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ ì •ìƒ ë¡œë“œ\n",
      "   XGBoost ë²„ì „: 3.0.2\n",
      "   Scikit-learn ë²„ì „: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "# ì…€ 2: ì„¤ì¹˜ í™•ì¸\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    import sklearn\n",
    "    import pulp\n",
    "    import folium\n",
    "    import geopy\n",
    "    print(\"âœ… ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ ì •ìƒ ë¡œë“œ\")\n",
    "    print(f\"   XGBoost ë²„ì „: {xgb.__version__}\")\n",
    "    print(f\"   Scikit-learn ë²„ì „: {sklearn.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ íŒ¨í‚¤ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "setup_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ğŸš€ ì „ê¸°ì°¨ ì¶©ì „ì†Œ ìµœì  ì…ì§€ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n",
      "ğŸ“Š ycnham + mg ë¸Œëœì¹˜ í†µí•© ë²„ì „\n"
     ]
    }
   ],
   "source": [
    "# ìë™ ë¦¬ë¡œë“œ ì„¤ì •\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"ğŸš€ ì „ê¸°ì°¨ ì¶©ì „ì†Œ ìµœì  ì…ì§€ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n",
    "print(\"ğŸ“Š ycnham + mg ë¸Œëœì¹˜ í†µí•© ë²„ì „\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imports_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: d:\\projects\\2025-1\\ë¹…ë°ì´í„°í”„ë¡œê·¸ë˜ë°\\2025-1-Big-Data-Programming\n",
      "ğŸ—‚ï¸ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: d:\\projects\\2025-1\\ë¹…ë°ì´í„°í”„ë¡œê·¸ë˜ë°\\2025-1-Big-Data-Programming\\scripts\n",
      "âš™ï¸ ëª¨ë¸ë§ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ê³µí†µ íŒ¨í‚¤ì§€ import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# src ê²½ë¡œ ì¶”ê°€\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\"..\"))\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, \"src\")\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.append(SRC_DIR)\n",
    "\n",
    "# seaborn ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •\n",
    "DATA_DIR = \"../data/processed\"\n",
    "RAW_DIR = \"../data/raw\"\n",
    "OUTPUT_IMG = \"../outputs/image\"\n",
    "OUTPUT_DIR = \"../data/modeling\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ëª¨ë¸ë§ ì„¤ì •\n",
    "KMEANS_MODE = \"auto\"  # auto, manual\n",
    "KMEANS_MANUAL_K = 5\n",
    "XGB_N_ESTIMATORS = 100\n",
    "COVERAGE_RADIUS_KM = 0.3  # 300m\n",
    "FACILITY_LIMIT = 280\n",
    "\n",
    "print(f\"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ—‚ï¸ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "print(f\"âš™ï¸ ëª¨ë¸ë§ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "import_unified_modules",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í†µí•©ëœ modeling_data_prep ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ\n",
      "âœ… ëª¨ë“  ë¶„ì„ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ\n",
      "   - í†µí•© ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ (ì´ì¤‘ ë°±ì—…)\n",
      "   - K-Means í´ëŸ¬ìŠ¤í„°ë§\n",
      "   - XGBoost ì˜ˆì¸¡ ëª¨ë¸\n",
      "   - MCLP ìµœì í™”\n",
      "   - ì „ëµ í‰ê°€ ë° ì‹œê°í™”\n"
     ]
    }
   ],
   "source": [
    "# í†µí•©ëœ ì „ì²˜ë¦¬ ëª¨ë“ˆ ë° ê¸°ì¡´ ëª¨ë“ˆë“¤ ì„í¬íŠ¸\n",
    "try:\n",
    "    # í†µí•©ëœ ì „ì²˜ë¦¬ ëª¨ë“ˆ\n",
    "    from preprocessing.modeling_data_prep import (\n",
    "        ModelingDataPreprocessor,\n",
    "        ImprovedDemandScoreCalculator,\n",
    "        create_modeling_preprocessor,\n",
    "        test_modeling_functions\n",
    "    )\n",
    "    print(\"âœ… í†µí•©ëœ modeling_data_prep ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ\")\n",
    "    \n",
    "    # ê¸°ì¡´ ë¶„ì„ ëª¨ë“ˆë“¤\n",
    "    from preprocessing.map_stations_to_grid import map_stations_to_grid\n",
    "    from evaluation.grid_coverage_eval import evaluate_installed_coverage, evaluate_grid_coverage\n",
    "    from modeling.kmeans_runner import generate_kmeans_features\n",
    "    from modeling.xgboost_model import train_and_predict\n",
    "    from modeling.mclp_model import solve_mclp\n",
    "    from evaluation.strategy_comparator import evaluate_strategy\n",
    "    from evaluation.new_coverage_analyzer import analyze_new_coverage, compute_percentiles\n",
    "    from visualization.map_plotter import plot_strategy_map\n",
    "    \n",
    "    print(\"âœ… ëª¨ë“  ë¶„ì„ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ\")\n",
    "    print(\"   - í†µí•© ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ (ì´ì¤‘ ë°±ì—…)\")\n",
    "    print(\"   - K-Means í´ëŸ¬ìŠ¤í„°ë§\")\n",
    "    print(\"   - XGBoost ì˜ˆì¸¡ ëª¨ë¸\")\n",
    "    print(\"   - MCLP ìµœì í™”\")\n",
    "    print(\"   - ì „ëµ í‰ê°€ ë° ì‹œê°í™”\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ”§ ê²½ë¡œ ì„¤ì •ê³¼ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_unified_system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "==================================================\n",
      "ğŸ§ª í†µí•© ëª¨ë¸ë§ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "ğŸ”§ í†µí•© ëª¨ë¸ë§ ì „ì²˜ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "   ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬: d:\\projects\\2025-1\\ë¹…ë°ì´í„°í”„ë¡œê·¸ë˜ë°\\2025-1-Big-Data-Programming\\data\\processed\n",
      "   ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: d:\\projects\\2025-1\\ë¹…ë°ì´í„°í”„ë¡œê·¸ë˜ë°\\2025-1-Big-Data-Programming\\data\\processed\n",
      "âœ… ModelingDataPreprocessor ìƒì„± ì„±ê³µ\n",
      "âœ… ImprovedDemandScoreCalculator ìƒì„± ì„±ê³µ\n",
      "ğŸ”§ prepare_modeling_data() í•¨ìˆ˜ í…ŒìŠ¤íŠ¸...\n",
      "ğŸš€ í†µí•© ëª¨ë¸ë§ ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰...\n",
      "ğŸ”§ í†µí•© ëª¨ë¸ë§ ì „ì²˜ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "   ğŸ“ ì…ë ¥ ë””ë ‰í† ë¦¬: d:\\projects\\2025-1\\ë¹…ë°ì´í„°í”„ë¡œê·¸ë˜ë°\\2025-1-Big-Data-Programming\\data\\processed\n",
      "   ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: d:\\projects\\2025-1\\ë¹…ë°ì´í„°í”„ë¡œê·¸ë˜ë°\\2025-1-Big-Data-Programming\\data\\processed\n",
      "ğŸš€ í†µí•© ëª¨ë¸ë§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\n",
      "\n",
      "1ï¸âƒ£ ê²©ì ì‹œìŠ¤í…œ ë°ì´í„° ì¤€ë¹„...\n",
      "   ğŸ“Š ê¸°ì¡´ ê²©ì íŒŒì¼ ë°œê²¬: 6,030í–‰\n",
      "   ğŸ“ˆ ìˆ˜ìš” ê²©ì: 2,029ê°œ\n",
      "   ğŸ“¦ ê³µê¸‰ ê²©ì: 6,030ê°œ\n",
      "   âœ… ê²©ì ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n",
      "\n",
      "2ï¸âƒ£ ê²©ì íŠ¹ì„± ë°ì´í„° ìƒì„± (ê°œì„ ëœ ìˆ˜ìš” ì ìˆ˜ í¬í•¨)...\n",
      "   ğŸ“Š ê²©ì ë°ì´í„° ë¡œë”©: 6,030í–‰\n",
      "   ğŸ”„ ê°œì„ ëœ ìˆ˜ìš” ì ìˆ˜ ê³„ì‚°ê¸° ë¡œë”©...\n",
      "   âœ… ì¶©ì „ ë°ì´í„° ë¡œë”©: 33,670í–‰\n",
      "   ğŸ“Š ëŒ€ìš©ëŸ‰ ìƒì—…ì‹œì„¤ ë°ì´í„° ì²­í¬ ë¡œë”© (í¬ê¸°: 273.3MB)\n",
      "   âœ… ìƒì—…ì‹œì„¤ ë°ì´í„° ë¡œë”©: 100,000í–‰\n",
      "   âœ… ì „ê¸°ì°¨ ë“±ë¡ ë°ì´í„° ë¡œë”©: 16í–‰\n",
      "   ğŸ“ˆ ì‹œê°„ íŒ¨í„´ ë³´ì • ê³„ìˆ˜: 1.10\n",
      "   ì§„í–‰ë¥ : 16.6% (1,000/6,030)\n",
      "   ì§„í–‰ë¥ : 33.2% (2,000/6,030)\n",
      "   ì§„í–‰ë¥ : 49.8% (3,000/6,030)\n",
      "   ì§„í–‰ë¥ : 66.3% (4,000/6,030)\n",
      "   ì§„í–‰ë¥ : 82.9% (5,000/6,030)\n",
      "   ì§„í–‰ë¥ : 99.5% (6,000/6,030)\n",
      "   ğŸ’¾ ê°œì„ ëœ ê²©ì íŠ¹ì„± íŒŒì¼ ì €ì¥: d:\\projects\\2025-1\\ë¹…ë°ì´í„°í”„ë¡œê·¸ë˜ë°\\2025-1-Big-Data-Programming\\data\\processed\\grid_features.csv\n",
      "   ğŸ“Š ì´ ê²©ì: 6,030ê°œ\n",
      "   ğŸ“Š í‰ê·  ìˆ˜ìš” ì ìˆ˜: 32.04\n",
      "   ğŸ“Š ìˆ˜ìš” ì ìˆ˜ ë²”ìœ„: 22.05 ~ 73.22\n",
      "   ğŸ“Š 0ì´ ì•„ë‹Œ ìˆ˜ìš” ê²©ì: 6,030ê°œ\n",
      "   ğŸ“Š í‰ê·  ìƒì—…ì‹œì„¤ ìˆ˜: 66.0ê°œ\n",
      "   ğŸ“Š í‰ê·  ì¶©ì „ì†Œ ìˆ˜: 1.0ê°œ\n",
      "   ğŸ“ˆ ìˆ˜ìš” ì ìˆ˜ ë¶„í¬:\n",
      "      ğŸ”¹ 0-20: 0ê°œ\n",
      "      ğŸ”¹ 20-40: 4,144ê°œ\n",
      "      ğŸ”¹ 40-60: 1,439ê°œ\n",
      "      ğŸ”¹ 60-80: 447ê°œ\n",
      "      ğŸ”¹ 80-100: 0ê°œ\n",
      "   âœ… ê°œì„ ëœ ê²©ì íŠ¹ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ\n",
      "\n",
      "3ï¸âƒ£ ê¸°ì¡´ ë°©ì‹ ê²©ì íŠ¹ì„± ë°ì´í„° ë°±ì—… ìƒì„±...\n",
      "   ğŸ“Š ê²©ì ë°ì´í„° ë¡œë”© (ê¸°ì¡´ ë°©ì‹): 6,030í–‰\n",
      "   ì§„í–‰ë¥ : 16.6% (1,000/6,030)\n"
     ]
    }
   ],
   "source": [
    "# í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë° ì „ì²˜ë¦¬ê¸° ìƒì„±\n",
    "print(\"ğŸ§ª í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # í†µí•© ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    test_results = test_modeling_functions()\n",
    "    \n",
    "    print(\"\\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "    for test_name, result in test_results.items():\n",
    "        status = \"âœ…\" if result else \"âŒ\"\n",
    "        print(f\"   {status} {test_name}\")\n",
    "    \n",
    "    # í†µí•© ì „ì²˜ë¦¬ê¸° ìƒì„±\n",
    "    print(\"\\nğŸ—ï¸ í†µí•© ì „ì²˜ë¦¬ê¸° ìƒì„± ì¤‘...\")\n",
    "    unified_preprocessor = create_modeling_preprocessor()\n",
    "    \n",
    "    if unified_preprocessor:\n",
    "        print(f\"âœ… ì „ì²˜ë¦¬ê¸° ìƒì„± ì„±ê³µ: {type(unified_preprocessor).__name__}\")\n",
    "        \n",
    "        # ì „ì²˜ë¦¬ê¸° ì •ë³´ ì¶œë ¥\n",
    "        if hasattr(unified_preprocessor, 'get_info'):\n",
    "            info = unified_preprocessor.get_info()\n",
    "            print(\"\\nğŸ“‹ ì „ì²˜ë¦¬ê¸° ì •ë³´:\")\n",
    "            for key, value in info.items():\n",
    "                print(f\"   â€¢ {key}: {value}\")\n",
    "    else:\n",
    "        print(\"âŒ ì „ì²˜ë¦¬ê¸° ìƒì„± ì‹¤íŒ¨ - ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "        unified_preprocessor = ModelingDataPreprocessor()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ”„ ê¸°ë³¸ ì•ˆì „ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "    unified_preprocessor = ModelingDataPreprocessor()\n",
    "    print(\"âœ… ê¸°ë³¸ ì „ì²˜ë¦¬ê¸°ë¡œ ì´ˆê¸°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_title",
   "metadata": {},
   "source": [
    "# 1. í†µí•© ì „ì²˜ë¦¬ ë‹¨ê³„\n",
    "\n",
    "### ì „ê¸°ì°¨ ì¶©ì „ì†Œ ìœ„ì¹˜ ë§¤í•‘ ë° ê²©ì ì‹œìŠ¤í…œ ì ìš© + í†µí•© ëª¨ë¸ë§ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "map_stations_and_prepare_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ë‹¨ê³„: ìœ„ì¹˜ ë§¤í•‘ ì‹¤í–‰\n",
    "print(\"ğŸ“ 1ë‹¨ê³„: ì „ê¸°ì°¨ ì¶©ì „ì†Œ ìœ„ì¹˜ ë§¤í•‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    map_stations_to_grid(\n",
    "        env_path=os.path.join(RAW_DIR, \"í•œêµ­í™˜ê²½ê³µë‹¨_ì „ê¸°ì°¨ ì¶©ì „ì†Œ ìœ„ì¹˜ ë° ìš´ì˜ì •ë³´(ì¶©ì „ì†Œ ID í¬í•¨)_20230531.csv\"),\n",
    "        grid_path=os.path.join(DATA_DIR, \"grid_system_processed.csv\"),\n",
    "        output_path=os.path.join(DATA_DIR, \"charging_stations_seoul_gridded.csv\")\n",
    "    )\n",
    "    print(\"âœ… ì¶©ì „ì†Œ ìœ„ì¹˜ ë§¤í•‘ ì™„ë£Œ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ìœ„ì¹˜ ë§¤í•‘ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# 2ë‹¨ê³„: í†µí•© ëª¨ë¸ë§ ë°ì´í„° ì¤€ë¹„\n",
    "print(\"\\nâš™ï¸ 2ë‹¨ê³„: í†µí•© ëª¨ë¸ë§ ë°ì´í„° ì „ì²˜ë¦¬\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "    print(\"ğŸ“‚ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    original_data = unified_preprocessor.load_training_data()\n",
    "    \n",
    "    if original_data is not None and not original_data.empty:\n",
    "        print(f\"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {original_data.shape}\")\n",
    "        \n",
    "        # í†µí•© ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "        print(\"ğŸ”„ í†µí•© ì „ì²˜ë¦¬ ì‹¤í–‰ ì¤‘...\")\n",
    "        processed_data = unified_preprocessor.preprocess_for_modeling(original_data)\n",
    "        \n",
    "        if processed_data is not None and not processed_data.empty:\n",
    "            print(f\"âœ… í†µí•© ì „ì²˜ë¦¬ ì™„ë£Œ: {processed_data.shape}\")\n",
    "            \n",
    "            # ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼ í™•ì¸\n",
    "            new_columns = set(processed_data.columns) - set(original_data.columns)\n",
    "            if new_columns:\n",
    "                print(f\"ğŸ†• ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼: {len(new_columns)}ê°œ\")\n",
    "                for col in sorted(list(new_columns)[:5]):\n",
    "                    print(f\"   â€¢ {col}\")\n",
    "                if len(new_columns) > 5:\n",
    "                    print(f\"   â€¢ ... ì™¸ {len(new_columns)-5}ê°œ\")\n",
    "            \n",
    "            # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            unified_output_path = os.path.join(DATA_DIR, f\"modeling_data_unified_{timestamp}.csv\")\n",
    "            processed_data.to_csv(unified_output_path, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # ìµœì‹  ë²„ì „ìœ¼ë¡œë„ ì €ì¥ (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í˜¸í™˜ì„±)\n",
    "            latest_path = os.path.join(DATA_DIR, \"grid_features.csv\")\n",
    "            processed_data.to_csv(latest_path, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            print(f\"ğŸ’¾ í†µí•© ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥: {unified_output_path}\")\n",
    "            print(f\"ğŸ’¾ í˜¸í™˜ì„± íŒŒì¼ ì—…ë°ì´íŠ¸: {latest_path}\")\n",
    "        else:\n",
    "            print(\"âŒ í†µí•© ì „ì²˜ë¦¬ ì‹¤íŒ¨\")\n",
    "    else:\n",
    "        print(\"âŒ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í†µí•© ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ğŸ”„ ê¸°ì¡´ íŒŒì¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\nğŸ‰ í†µí•© ì „ì²˜ë¦¬ ë‹¨ê³„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing_analysis_title",
   "metadata": {},
   "source": [
    "# 2. ê¸°ì¡´ ì¶©ì „ì†Œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing_station_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ ì¶©ì „ì†Œ ê¸°ë°˜ ì„¤ì¹˜ ì»¤ë²„ìœ¨ ë¶„ì„\n",
    "print(\"ğŸ“Š ê¸°ì¡´ ì¶©ì „ì†Œ ì»¤ë²„ìœ¨ ë¶„ì„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # ê²½ë¡œ ì„¤ì •\n",
    "    GRID_FEATURES_PATH = os.path.join(DATA_DIR, \"grid_features.csv\")\n",
    "    INSTALLED_STATION_PATH = os.path.join(DATA_DIR, \"charging_stations_seoul_gridded.csv\")\n",
    "    \n",
    "    # í‰ê°€ ì‹¤í–‰\n",
    "    coverage_result = evaluate_installed_coverage(\n",
    "        grid_path=GRID_FEATURES_PATH,\n",
    "        station_path=INSTALLED_STATION_PATH,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ê¸°ì¡´ ì¶©ì „ì†Œ ë¶„ì„ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê¸°ì¡´ ì¶©ì „ì†Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_title",
   "metadata": {},
   "source": [
    "# 3. K-Means í´ëŸ¬ìŠ¤í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ K-Means í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # ê²½ë¡œ ì„¤ì •\n",
    "    GRID_PROCESSED_PATH = os.path.join(DATA_DIR, \"grid_system_processed.csv\")\n",
    "    FEATURES_ALL_PATH = os.path.join(DATA_DIR, \"grid_features.csv\")\n",
    "    KMEANS_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"kmeans_grid_features.csv\")\n",
    "    \n",
    "    # KMeans ì‹¤í–‰\n",
    "    features_kmeans, used_k = generate_kmeans_features(\n",
    "        grid_path=GRID_PROCESSED_PATH,\n",
    "        features_path=FEATURES_ALL_PATH,\n",
    "        output_path=KMEANS_OUTPUT_PATH,\n",
    "        mode=KMEANS_MODE,\n",
    "        manual_k=KMEANS_MANUAL_K,\n",
    "        return_top_cluster_only=True\n",
    "    )\n",
    "    \n",
    "    # ì»¤ë²„ìœ¨ í‰ê°€\n",
    "    all_df = pd.read_csv(FEATURES_ALL_PATH)\n",
    "    kmeans_result = evaluate_grid_coverage(\n",
    "        df_selected=features_kmeans,\n",
    "        df_all=all_df,\n",
    "        demand_col=\"demand_score\",\n",
    "        label=\"í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… K-Means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ K-Means í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgboost_title",
   "metadata": {},
   "source": [
    "# 4. XGBoost ì˜ˆì¸¡ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xgboost_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¤– XGBoost ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # 1. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¡œ í•™ìŠµ\n",
    "    print(\"1ï¸âƒ£ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ\")\n",
    "    XGB_INPUT_PATH = os.path.join(OUTPUT_DIR, \"kmeans_grid_features.csv\")\n",
    "    features = pd.read_csv(XGB_INPUT_PATH)\n",
    "    \n",
    "    # í•™ìŠµ feature ì§€ì •\n",
    "    selected_features = [\n",
    "        'supply_score',\n",
    "        'station_count',\n",
    "        'commercial_count',\n",
    "        'supply_demand_ratio',\n",
    "        'population_density',\n",
    "        'accessibility_score',\n",
    "        'transport_score',\n",
    "        'cluster'\n",
    "    ]\n",
    "    \n",
    "    # cluster ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ë²”ì£¼í˜• ì²˜ë¦¬\n",
    "    if 'cluster' in features.columns:\n",
    "        features['cluster'] = features['cluster'].astype('category')\n",
    "    \n",
    "    # ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "    features_with_pred, metrics, model = train_and_predict(\n",
    "        df=features,\n",
    "        features=selected_features,\n",
    "        label='demand_score',\n",
    "        n_estimators=XGB_N_ESTIMATORS,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "    XGB_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"xgboost_grid_features.csv\")\n",
    "    features_with_pred.to_csv(XGB_OUTPUT_PATH, index=False)\n",
    "    print(f\"ğŸ’¾ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {XGB_OUTPUT_PATH}\")\n",
    "    \n",
    "    # 2. ì „ì²´ ë°ì´í„°ë¡œ ì˜ˆì¸¡ (í‰ê°€ìš©)\n",
    "    print(\"\\n2ï¸âƒ£ ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ (í‰ê°€ìš©)\")\n",
    "    INPUT_PATH = os.path.join(DATA_DIR, \"grid_features.csv\")\n",
    "    OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"xgboost_grid_features_test.csv\")\n",
    "    \n",
    "    features_all = pd.read_csv(INPUT_PATH)\n",
    "    \n",
    "    # ì „ì²´ ë°ì´í„°ìš© í”¼ì²˜ (cluster ì œì™¸)\n",
    "    test_features = [\n",
    "        'supply_score',\n",
    "        'station_count',\n",
    "        'commercial_count',\n",
    "        'supply_demand_ratio',\n",
    "        'population_density',\n",
    "        'accessibility_score',\n",
    "        'transport_score'\n",
    "    ]\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    features_with_pred_all, metrics_all, model_all = train_and_predict(\n",
    "        df=features_all,\n",
    "        features=test_features,\n",
    "        label='demand_score',\n",
    "        n_estimators=XGB_N_ESTIMATORS,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    features_with_pred_all.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"ğŸ’¾ ì „ì²´ ë°ì´í„° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {OUTPUT_PATH}\")\n",
    "    \n",
    "    print(\"âœ… XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mclp_title",
   "metadata": {},
   "source": [
    "# 5. MCLP ìµœì í™”\n",
    "\n",
    "## MCLP ëª¨ë¸ ìµœì í™” ë° ì„¤ì • ì´ìœ \n",
    "\n",
    "### 1. ì™œ 300m ë°˜ê²½ì„ ì„¤ì •í–ˆëŠ”ê°€?\n",
    "ì „ê¸°ì°¨ ì¶©ì „ì†Œì˜ **ì»¤ë²„ë¦¬ì§€ë¥¼ í‰ê°€**í•˜ëŠ” ë° ìˆì–´ ê° ì¶©ì „ì†Œê°€ **íš¨ìœ¨ì ìœ¼ë¡œ ì„œë¹„ìŠ¤í•  ìˆ˜ ìˆëŠ” ë²”ìœ„**ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì‹¤í—˜ì„ í†µí•´ ì—¬ëŸ¬ ë°˜ê²½ì„ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼, **0.3km (300m)** ë°˜ê²½ì´ ìµœì ì˜ ì»¤ë²„ë¦¬ì§€ë¥¼ ì œê³µí•œë‹¤ê³  íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 2. ì™œ 280ê°œì˜ ì¶©ì „ì†Œ ì„¤ì¹˜ìˆ˜ë¥¼ ì„¤ì •í–ˆëŠ”ê°€?\n",
    "MCLP ëª¨ë¸ì—ì„œ **ìµœì†Œ ì„¤ì¹˜ ìˆ˜ë¡œ ëª©í‘œ ì»¤ë²„ìœ¨ì„ ë‹¬ì„±**í•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ëª©í‘œ ì»¤ë²„ìœ¨ì„ **69.39%**ë¡œ ì„¤ì •í•˜ê³ , ì´ë¥¼ ë§Œì¡±í•˜ëŠ” **ìµœì†Œ ì„¤ì¹˜ ìˆ˜**ë¥¼ ì°¾ê¸° ìœ„í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•œ ê²°ê³¼, **280ê°œ**ì˜ ì¶©ì „ì†Œê°€ **70.08%**ì˜ ì»¤ë²„ë¦¬ì§€ë¥¼ ì œê³µí•œë‹¤ê³  ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mclp_optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ MCLP ìµœì í™” ì‹¤í–‰\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    XGB_INPUT_PATH = os.path.join(OUTPUT_DIR, \"xgboost_grid_features.csv\")\n",
    "    features_with_pred = pd.read_csv(XGB_INPUT_PATH)\n",
    "    \n",
    "    print(f\"ğŸ“ ì„¤ì •: ë°˜ê²½ {COVERAGE_RADIUS_KM}km, ìµœëŒ€ {FACILITY_LIMIT}ê°œ ì¶©ì „ì†Œ\")\n",
    "    \n",
    "    # MCLP ì‹¤í–‰\n",
    "    final_df, final_summary, _ = solve_mclp(\n",
    "        df=features_with_pred,\n",
    "        coverage_radius=COVERAGE_RADIUS_KM,\n",
    "        facility_limit=FACILITY_LIMIT,\n",
    "        demand_column='predicted_demand_score',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # ì €ì¥ ê²½ë¡œ ì§€ì •\n",
    "    MCLP_ALL_PATH = os.path.join(OUTPUT_DIR, \"mclp_grid_features.csv\")\n",
    "    MCLP_SELECTED_PATH = os.path.join(OUTPUT_DIR, \"mclp_selected_grid_features.csv\")\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    final_df.to_csv(MCLP_ALL_PATH, index=False)\n",
    "    final_df[final_df['selected'] == 1].to_csv(MCLP_SELECTED_PATH, index=False)\n",
    "    \n",
    "    # ìš”ì•½ ì¶œë ¥\n",
    "    print(\"\\nğŸ“Š MCLP ìµœì í™” ê²°ê³¼:\")\n",
    "    for k, v in final_summary.items():\n",
    "        print(f\"   â€¢ {k}: {v:.2f}\")\n",
    "    \n",
    "    print(f\"ğŸ’¾ MCLP ê²°ê³¼ ì €ì¥ ì™„ë£Œ\")\n",
    "    print(\"âœ… MCLP ìµœì í™” ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ MCLP ìµœì í™” ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_title",
   "metadata": {},
   "source": [
    "# 6. ì „ëµ í‰ê°€ ë° ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategy_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š ì „ëµë³„ ì„±ëŠ¥ í‰ê°€\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    stations = pd.read_csv(os.path.join(DATA_DIR, \"charging_stations_seoul_gridded.csv\"))\n",
    "    features_with_pred = pd.read_csv(os.path.join(OUTPUT_DIR, \"xgboost_grid_features_test.csv\"))\n",
    "    kmeans_features = pd.read_csv(os.path.join(OUTPUT_DIR, \"kmeans_grid_features.csv\"))\n",
    "    mclp_features = pd.read_csv(os.path.join(OUTPUT_DIR, \"mclp_grid_features.csv\"))\n",
    "    \n",
    "    # ì „ëµë³„ ê²©ì ì…‹ ì •ì˜\n",
    "    strategy_sets = {\n",
    "        \"ê¸°ì¡´ ì¶©ì „ì†Œ ì „ì²´\": set(stations['grid_id']),\n",
    "        \"ëœë¤ ì„¤ì¹˜\": set(stations['grid_id'].drop_duplicates().sample(n=526, random_state=42)),\n",
    "        \"í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì„¤ì¹˜\": set(kmeans_features['grid_id'].dropna()),\n",
    "        \"MCLP ì¶”ì²œ ì„¤ì¹˜\": set(mclp_features[mclp_features['selected'] == 1]['grid_id']),\n",
    "    }\n",
    "    \n",
    "    # ì „ëµë³„ í‰ê°€ ì‹¤í–‰\n",
    "    print(\"ğŸ” ê° ì „ëµë³„ ì„±ëŠ¥ ë¶„ì„:\")\n",
    "    for label, grid_set in strategy_sets.items():\n",
    "        evaluate_strategy(label, grid_set, features_with_pred)\n",
    "    \n",
    "    print(\"âœ… ì „ëµë³„ í‰ê°€ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì „ëµ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new_coverage_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ†• ì‹ ê·œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # ì‹ ê·œ ì»¤ë²„ ë¶„ì„\n",
    "    results, uncovered = analyze_new_coverage(\n",
    "        features_with_pred, \n",
    "        strategy_sets, \n",
    "        base_label=\"ê¸°ì¡´ ì¶©ì „ì†Œ ì „ì²´\"\n",
    "    )\n",
    "    \n",
    "    # ë°±ë¶„ìœ„ ê³„ì‚°\n",
    "    percentile_dfs = []\n",
    "    for label, info in results.items():\n",
    "        df = compute_percentiles(features_with_pred, info['new_grids'], label=label)\n",
    "        percentile_dfs.append(df)\n",
    "    \n",
    "    # ê²°ê³¼ í†µí•© ë° ì¶œë ¥\n",
    "    if percentile_dfs:\n",
    "        final_df = pd.concat(percentile_dfs).sort_values(by='rank').reset_index(drop=True)\n",
    "        print(\"\\nğŸ“‹ ì‹ ê·œ ì»¤ë²„ ê²©ì ìƒìœ„ ìˆ˜ìš” ë¹„ìœ¨ ë¶„ì„:\")\n",
    "        print(final_df.head(20).to_string(index=False))\n",
    "        \n",
    "        # ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "        analysis_output_path = os.path.join(OUTPUT_DIR, \"new_coverage_analysis.csv\")\n",
    "        final_df.to_csv(analysis_output_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ ì‹ ê·œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_output_path}\")\n",
    "    else:\n",
    "        print(\"ğŸ“Š ì‹ ê·œ ì»¤ë²„ë¦¬ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        final_df = pd.DataFrame()\n",
    "    \n",
    "    print(\"âœ… ì‹ ê·œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì‹ ê·œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_title",
   "metadata": {},
   "source": [
    "# 7. ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ—ºï¸ ê²°ê³¼ ì‹œê°í™” ìƒì„±\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # ì§€ë„ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "    MAP_OUTPUT_DIR = \"../outputs/maps\"\n",
    "    os.makedirs(MAP_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    if not final_df.empty:\n",
    "        MAP_OUTPUT_PATH = os.path.join(MAP_OUTPUT_DIR, \"new_covered_grids_map.html\")\n",
    "        \n",
    "        # ì§€ë„ ì‹œê°í™” ì‹¤í–‰\n",
    "        plot_strategy_map(\n",
    "            final_df=final_df,\n",
    "            coord_df=features_with_pred,\n",
    "            output_path=MAP_OUTPUT_PATH\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ—ºï¸ ì§€ë„ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {MAP_OUTPUT_PATH}\")\n",
    "    else:\n",
    "        print(\"ğŸ“Š ì‹œê°í™”í•  ì‹ ê·œ ì»¤ë²„ë¦¬ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_title",
   "metadata": {},
   "source": [
    "# 8. ìµœì¢… ìš”ì•½ ë° ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ ë¶„ì„ ì™„ë£Œ ìš”ì•½\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # ë¶„ì„ ê²°ê³¼ ìš”ì•½\n",
    "    print(\"\\nğŸ¯ **ì£¼ìš” ë¶„ì„ ê²°ê³¼**\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 1. í†µí•© ì „ì²˜ë¦¬ ê²°ê³¼\n",
    "    if 'processed_data' in locals() and processed_data is not None:\n",
    "        print(f\"1ï¸âƒ£ í†µí•© ì „ì²˜ë¦¬: {processed_data.shape[0]:,}ê°œ ê²©ì Ã— {processed_data.shape[1]}ê°œ íŠ¹ì„±\")\n",
    "        \n",
    "        # ì‚¬ìš©ëœ ì „ì²˜ë¦¬ê¸° ì •ë³´\n",
    "        preprocessor_type = type(unified_preprocessor).__name__\n",
    "        if preprocessor_type == \"ImprovedDemandScoreCalculator\":\n",
    "            print(\"   âœ… ê³ ê¸‰ ìˆ˜ìš” ì ìˆ˜ ê³„ì‚° (ì—…ì¢…ë³„ ê°€ì¤‘ì¹˜ ì ìš©)\")\n",
    "        else:\n",
    "            print(\"   âœ… ì•ˆì •ì ì¸ ê¸°ë³¸ ì „ì²˜ë¦¬\")\n",
    "    \n",
    "    # 2. ëª¨ë¸ë§ ê²°ê³¼\n",
    "    if 'metrics' in locals():\n",
    "        print(f\"\\n2ï¸âƒ£ XGBoost ëª¨ë¸ ì„±ëŠ¥:\")\n",
    "        print(f\"   â€¢ MAE: {metrics.get('mae', 'N/A'):.2f}\")\n",
    "        print(f\"   â€¢ RMSE: {metrics.get('rmse', 'N/A'):.2f}\")\n",
    "        print(f\"   â€¢ RÂ²: {metrics.get('r2', 'N/A'):.4f}\")\n",
    "    \n",
    "    # 3. ìµœì í™” ê²°ê³¼\n",
    "    if 'final_summary' in locals():\n",
    "        print(f\"\\n3ï¸âƒ£ MCLP ìµœì í™” ê²°ê³¼:\")\n",
    "        print(f\"   â€¢ ì„ íƒëœ ì„¤ì¹˜ì§€: {final_summary.get('selected_count', 'N/A'):.0f}ê°œ\")\n",
    "        print(f\"   â€¢ ì»¤ë²„ë¦¬ì§€: {final_summary.get('coverage_rate', 'N/A'):.2f}%\")\n",
    "        print(f\"   â€¢ ì„¤ì¹˜ íš¨ìœ¨: {final_summary.get('demand_satisfaction_ratio', 'N/A'):.2f}\")\n",
    "    \n",
    "    # 4. ì „ëµ ë¹„êµ ìš”ì•½\n",
    "    print(f\"\\n4ï¸âƒ£ ì „ëµë³„ ë¹„êµ (ìƒìœ„ 3ê°œ):\")\n",
    "    strategy_efficiency = {\n",
    "        \"MCLP ì¶”ì²œ\": \"ìµœê³  íš¨ìœ¨ (1,754.49)\",\n",
    "        \"í´ëŸ¬ìŠ¤í„° ê¸°ë°˜\": \"ë†’ì€ ì»¤ë²„ìœ¨ (68.47%)\",\n",
    "        \"ê¸°ì¡´ ì¶©ì „ì†Œ\": \"ê´‘ë²”ìœ„ ì»¤ë²„ (95.40%)\"\n",
    "    }\n",
    "    \n",
    "    for i, (strategy, desc) in enumerate(strategy_efficiency.items(), 1):\n",
    "        print(f\"   {i}. {strategy}: {desc}\")\n",
    "    \n",
    "    # 5. ìƒì„±ëœ íŒŒì¼ ëª©ë¡\n",
    "    print(f\"\\nğŸ“ **ìƒì„±ëœ íŒŒì¼ë“¤**\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    output_files = [\n",
    "        \"data/processed/modeling_data_unified_*.csv (í†µí•© ì „ì²˜ë¦¬ ê²°ê³¼)\",\n",
    "        \"data/processed/charging_stations_seoul_gridded.csv (ì¶©ì „ì†Œ ìœ„ì¹˜ ë§¤í•‘)\",\n",
    "        \"data/modeling/kmeans_grid_features.csv (í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼)\",\n",
    "        \"data/modeling/xgboost_grid_features.csv (XGBoost ì˜ˆì¸¡)\",\n",
    "        \"data/modeling/mclp_grid_features.csv (MCLP ìµœì í™”)\",\n",
    "        \"data/modeling/new_coverage_analysis.csv (ì‹ ê·œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„)\",\n",
    "        \"outputs/maps/new_covered_grids_map.html (ê²°ê³¼ ì§€ë„)\"\n",
    "    ]\n",
    "    \n",
    "    for file_desc in output_files:\n",
    "        print(f\"   ğŸ“„ {file_desc}\")\n",
    "    \n",
    "    # 6. ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ\n",
    "    print(f\"\\nğŸš€ **ê¶Œì¥ ë‹¤ìŒ ë‹¨ê³„**\")\n",
    "    print(\"=\" * 20)\n",
    "    print(\"   1. ìƒì„±ëœ ì§€ë„ íŒŒì¼ë¡œ ê²°ê³¼ ì‹œê°ì  ê²€í† \")\n",
    "    print(\"   2. MCLP ì¶”ì²œ ìœ„ì¹˜ì˜ ì‹¤ì œ ì„¤ì¹˜ ê°€ëŠ¥ì„± ê²€í† \")\n",
    "    print(\"   3. ì¶”ê°€ ì œì•½ ì¡°ê±´(ì˜ˆì‚°, ë¶€ì§€ í™•ë³´) ë°˜ì˜í•œ ì¬ìµœì í™”\")\n",
    "    print(\"   4. ì‹¤ì œ ì¶©ì „ì†Œ ì´ìš© ë°ì´í„°ë¡œ ëª¨ë¸ ê²€ì¦\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ **í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!**\")\n",
    "    print(f\"   ycnham + mg ë¸Œëœì¹˜ í†µí•© ë²„ì „ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   ì´ì¤‘ ë°±ì—… ì‹œìŠ¤í…œìœ¼ë¡œ ì•ˆì •ì„±ì´ ë³´ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    print(\"âš ï¸ ë¶„ì„ì€ ì™„ë£Œë˜ì—ˆì§€ë§Œ ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\nâ° ë¶„ì„ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_guide_final",
   "metadata": {},
   "source": [
    "## ğŸ”§ ì‚¬ìš© ê°€ì´ë“œ ë° ë¬¸ì œí•´ê²°\n",
    "\n",
    "### í†µí•© ì‹œìŠ¤í…œ íŠ¹ì§•\n",
    "- **ì´ì¤‘ ë°±ì—…**: ê³ ê¸‰ ê¸°ëŠ¥ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì „í™˜\n",
    "- **ì™„ë²½í•œ í˜¸í™˜ì„±**: ê¸°ì¡´ ì½”ë“œì™€ 100% í˜¸í™˜\n",
    "- **ì ì§„ì  ì—…ê·¸ë ˆì´ë“œ**: í•„ìš”ì— ë”°ë¼ ê³ ê¸‰ ê¸°ëŠ¥ ì„ íƒì  ì‚¬ìš©\n",
    "\n",
    "### ë¬¸ì œ ë°œìƒ ì‹œ ëŒ€ì²˜ ë°©ë²•\n",
    "1. **í†µí•© ì „ì²˜ë¦¬ ì‹¤íŒ¨**: ìë™ìœ¼ë¡œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì „í™˜ë¨\n",
    "2. **ëª¨ë“ˆ ì„í¬íŠ¸ ì˜¤ë¥˜**: ê²½ë¡œ ì„¤ì • í™•ì¸ í›„ ì¬ì‹¤í–‰\n",
    "3. **ë©”ëª¨ë¦¬ ë¶€ì¡±**: ì²­í¬ í¬ê¸° ì¡°ì • ë˜ëŠ” ë°°ì¹˜ ì²˜ë¦¬ í™œìš©\n",
    "\n",
    "### ì„±ëŠ¥ ìµœì í™” íŒ\n",
    "- `ImprovedDemandScoreCalculator` ì‚¬ìš© ì‹œ ë” ì •êµí•œ ìˆ˜ìš” ë¶„ì„ ê°€ëŠ¥\n",
    "- ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ì²­í¬ ë‹¨ìœ„ë¡œ ìë™ ì²˜ë¦¬ë¨\n",
    "- ëª¨ë“  ì¤‘ê°„ ê²°ê³¼ê°€ ì €ì¥ë˜ì–´ ì¬ì‹¤í–‰ ì‹œ ì‹œê°„ ë‹¨ì¶• ê°€ëŠ¥\n",
    "\n",
    "### ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥ ì„¤ì •\n",
    "```python\n",
    "# ëª¨ë¸ë§ íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "KMEANS_MODE = \"manual\"  # ìˆ˜ë™ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„¤ì •\n",
    "KMEANS_MANUAL_K = 8     # í´ëŸ¬ìŠ¤í„° ìˆ˜\n",
    "XGB_N_ESTIMATORS = 200  # XGBoost íŠ¸ë¦¬ ìˆ˜\n",
    "COVERAGE_RADIUS_KM = 0.5  # ì»¤ë²„ë¦¬ì§€ ë°˜ê²½\n",
    "FACILITY_LIMIT = 500    # ìµœëŒ€ ì„¤ì¹˜ ê°€ëŠ¥ ìˆ˜\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
